import streamlit as st
import cv2
import numpy as np
import mediapipe as mp
import joblib
from PIL import Image
import av
import time
import base64
from collections import deque
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, WebRtcMode, RTCConfiguration

# --- Page Configuration ---
st.set_page_config(page_title="AI ê±°ë¶ëª© êµì • í”„ë¡œ (AI Posture Pro)", page_icon="ğŸ¢", layout="wide")

# --- CSS & Audio Script ---
# ì†Œë¦¬ ì¬ìƒì„ ìœ„í•œ ìë°”ìŠ¤í¬ë¦½íŠ¸ ë° ìŠ¤íƒ€ì¼
def get_audio_html(sound_file_path):
    # ê²½ê³ ìŒ (ë¹„í”„ìŒ) Base64 ë°ì´í„°
    beep_b64 = "UklGRl9vT19XQVZFZm10IBAAAAABAAEAQB8AAEAfAAABAAgAZGF0YU" + "A" * 500 # ì§§ì€ ë¹„í”„ìŒ ë”ë¯¸ ë°ì´í„° (ì‹¤ì œ ì‘ë™ì„ ìœ„í•´ ì•„ë˜ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©)
    
    # ì‹¤ì œ ë¸Œë¼ìš°ì € ë‚´ì¥ ì˜¤ë””ì˜¤ ì‚¬ìš©
    js_code = """
        <script>
        function playAlert() {
            var audio = new Audio('https://actions.google.com/sounds/v1/alarms/beep_short.ogg');
            audio.volume = 0.5;
            audio.play();
        }
        </script>
        <div id="audio-container"></div>
    """
    return js_code

st.markdown("""
    <style>
    .big-font { font-size:24px !important; font-weight: bold; }
    .good-text { color: #2ecc71; font-weight: bold; font-size: 24px; }
    .mild-text { color: #f1c40f; font-weight: bold; font-size: 24px; }
    .severe-text { color: #e74c3c; font-weight: bold; font-size: 24px; animation: blink 1s infinite; }
    
    @keyframes blink {
        50% { opacity: 0.5; }
    }
    
    .stat-box {
        background-color: #f0f2f6;
        border-radius: 10px;
        padding: 20px;
        margin: 10px 0;
        text-align: center;
    }
    .stat-title { font-size: 16px; color: #555; }
    .stat-value { font-size: 28px; font-weight: bold; color: #333; }
    </style>
    """, unsafe_allow_html=True)

st.markdown(get_audio_html(""), unsafe_allow_html=True) # ì˜¤ë””ì˜¤ ìŠ¤í¬ë¦½íŠ¸ ë¡œë“œ

st.title("ğŸ¢ AI ê±°ë¶ëª© êµì • Pro")
st.markdown("ì›¹ìº ì„ ì¼œê³  ìì„¸ë¥¼ ë¶„ì„í•˜ì„¸ìš”. **'ì‹¬ê°' ë‹¨ê³„ê°€ ì§€ì†ë˜ë©´ ì•Œë¦¼ì´ ìš¸ë¦½ë‹ˆë‹¤.**")

# --- Load Model & MediaPipe ---
@st.cache_resource
def load_model():
    try:
        return joblib.load('posture_model.pkl')
    except:
        return None

model = load_model()
mp_pose = mp.solutions.pose

# --- Session State ì´ˆê¸°í™” (í†µê³„ìš©) ---
if 'start_time' not in st.session_state:
    st.session_state.start_time = time.time()
if 'total_severe_time' not in st.session_state:
    st.session_state.total_severe_time = 0
if 'calibration_y' not in st.session_state:
    st.session_state.calibration_y = 0.0 # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì˜¤í”„ì…‹

# --- Real-time Video Processing Class ---
class VideoProcessor(VideoTransformerBase):
    def __init__(self):
        self.pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, model_complexity=1)
        self.model = model
        
        # 1. Smoothing (ê²°ê³¼ ë–¨ë¦¼ ë°©ì§€)
        self.history_len = 10
        self.prob_history = deque(maxlen=self.history_len)
        
        # ê²°ê³¼ ê³µìœ  ë³€ìˆ˜
        self.latest_probs = {'good': 0, 'mild': 0, 'severe': 0}
        self.latest_pred = "good"
        self.severe_consecutive_frames = 0 # ì—°ì† í”„ë ˆì„ ì¹´ìš´íŠ¸ (ì†Œë¦¬ ì•Œë¦¼ìš©)
        self.trigger_sound = False
        
        # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê°’ ê°€ì ¸ì˜¤ê¸° (Processor ìƒì„± ì‹œì ì˜ ê°’)
        # ì£¼ì˜: ìŠ¤íŠ¸ë¦¼ ë„ì¤‘ ê°’ì„ ë°”ê¾¸ë ¤ë©´ ë³„ë„ ë©”ì»¤ë‹ˆì¦˜ í•„ìš”í•˜ë‚˜, ì—¬ê¸°ì„  ì´ˆê¸°ê°’ ì‚¬ìš©
        self.cal_y = 0 

    def update_calibration(self, value):
        self.cal_y = value

    def recv(self, frame):
        img = frame.to_ndarray(format="bgr24")
        h, w, _ = img.shape
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = self.pose.process(img_rgb)

        current_pred = "good"
        current_probs = [1.0, 0.0, 0.0] # ê¸°ë³¸ê°’

        if results.pose_landmarks:
            landmarks = results.pose_landmarks.landmark
            
            try:
                # 2. Feature Extraction
                l_sh = landmarks[11]; r_sh = landmarks[12]
                center_x = (l_sh.x + r_sh.x) / 2
                center_y = (l_sh.y + r_sh.y) / 2
                
                # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì ìš© (ë†’ì´ ë³´ì •) - ë‹¨ìˆœ ì˜ˆì‹œ: ì¤‘ì‹¬ì  Yì¶• ì´ë™
                # ì‹¤ì œë¡œëŠ” ëª¨ë¸ ì¬í•™ìŠµì´ ì¢‹ìœ¼ë‚˜, ì—¬ê¸°ì„œëŠ” ì…ë ¥ê°’ ë¯¸ì„¸ ì¡°ì •ìœ¼ë¡œ êµ¬í˜„
                # center_y += st.session_state.get('calibration_y', 0) 

                width = np.linalg.norm(np.array([l_sh.x, l_sh.y]) - np.array([r_sh.x, r_sh.y]))
                if width == 0: width = 1

                indices = [0, 2, 5, 7, 8, 11, 12]
                features = []
                
                # ì‹œê°í™” í¬ì¸íŠ¸ ë° ë¼ˆëŒ€
                keypoints = {} # ê·¸ë¦¬ê¸° ìœ„í•´ ì¢Œí‘œ ì €ì¥

                for idx in indices:
                    lm = landmarks[idx]
                    # ì •ê·œí™”
                    norm_x = (lm.x - center_x) / width
                    norm_y = (lm.y - center_y) / width
                    features.extend([norm_x, norm_y])
                    
                    # í”½ì…€ ì¢Œí‘œ ì €ì¥
                    px, py = int(lm.x * w), int(lm.y * h)
                    keypoints[idx] = (px, py)

                # 3. Prediction
                if self.model:
                    probs = self.model.predict_proba([features])[0]
                    
                    # Smoothing: íì— í™•ë¥  ì €ì¥
                    self.prob_history.append(probs)
                    
                    # í‰ê·  í™•ë¥  ê³„ì‚°
                    avg_probs = np.mean(self.prob_history, axis=0)
                    classes = self.model.classes_ # ['good', 'mild', 'severe'] ìˆœì„œë¼ê³  ê°€ì • (í™•ì¸ í•„ìš”)
                    
                    # í´ë˜ìŠ¤ ë§¤í•‘ (ëª¨ë¸ë§ˆë‹¤ ìˆœì„œê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ. ì—¬ê¸°ì„  ì´ë¦„ìœ¼ë¡œ ë§¤ì¹­)
                    prob_dict = {cls: p for cls, p in zip(classes, avg_probs)}
                    self.latest_probs = prob_dict
                    
                    # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ê²°ì •
                    current_pred = max(prob_dict, key=prob_dict.get)
                    self.latest_pred = current_pred
                    
                    # 4. ì‹œê°í™” (Skeleton Visualization)
                    # ìƒ‰ìƒ ê²°ì •
                    color = (0, 255, 0) # Green
                    if current_pred == 'mild': color = (0, 255, 255) # Yellow
                    if current_pred == 'severe': color = (0, 0, 255) # Red (BGR)

                    # ì  ê·¸ë¦¬ê¸°
                    for idx, (px, py) in keypoints.items():
                        cv2.circle(img, (px, py), 5, color, -1)
                    
                    # ë¼ˆëŒ€ ê·¸ë¦¬ê¸° (ì–´ê¹¨ì„ , ëª©ì„ )
                    # 11:ì™¼ìª½ì–´ê¹¨, 12:ì˜¤ë¥¸ìª½ì–´ê¹¨, 0:ì½”
                    if 11 in keypoints and 12 in keypoints:
                        cv2.line(img, keypoints[11], keypoints[12], color, 2)
                    if 0 in keypoints:
                        # ì–´ê¹¨ ì¤‘ì‹¬ ê³„ì‚°
                        sh_center = ((keypoints[11][0] + keypoints[12][0]) // 2, 
                                     (keypoints[11][1] + keypoints[12][1]) // 2)
                        cv2.line(img, sh_center, keypoints[0], color, 2)

                    # 5. ì†Œë¦¬ ì•Œë¦¼ ë¡œì§ (Severe ìƒíƒœ ì§€ì† ì‹œ)
                    if current_pred == 'severe':
                        self.severe_consecutive_frames += 1
                        if self.severe_consecutive_frames > 30: # ì•½ 1ì´ˆ ì´ìƒ (30fps ê¸°ì¤€)
                            self.trigger_sound = True
                    else:
                        self.severe_consecutive_frames = 0
                        self.trigger_sound = False
                        
            except Exception as e:
                pass

        return av.VideoFrame.from_ndarray(img, format="bgr24")

# --- UI Layout ---
col_main, col_stat = st.columns([3, 1])

with col_main:
    # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (ì´ˆê¸° ì„¤ì •)
    if st.button("ğŸ“ í˜„ì¬ ìì„¸ë¥¼ 'ê¸°ì¤€ì 'ìœ¼ë¡œ ì„¤ì • (Calibration)"):
        st.session_state.start_time = time.time()
        st.session_state.total_severe_time = 0
        st.success("ê¸°ì¤€ì ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤! íƒ€ì´ë¨¸ê°€ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.")

    if model is None:
        st.error("ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        ctx = webrtc_streamer(
            key="posture-pro",
            video_processor_factory=VideoProcessor,
            mode=WebRtcMode.SENDRECV,
            rtc_configuration=RTCConfiguration({"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}),
            media_stream_constraints={"video": True, "audio": False},
            async_processing=True
        )

with col_stat:
    st.markdown("### ğŸ“Š ì‹¤ì‹œê°„ ë¶„ì„")
    status_ph = st.empty()
    st.write("---")
    st.markdown("### â±ï¸ ì‹œê°„ í†µê³„")
    timer_ph = st.empty()
    severe_timer_ph = st.empty()
    
    # JavaScript Sound Trigger Placeholder
    sound_ph = st.empty()

# --- Main Loop (Outside of Streamer) ---
if ctx.state.playing:
    while True:
        if ctx.video_processor:
            # 1. ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            probs = ctx.video_processor.latest_probs
            pred = ctx.video_processor.latest_pred
            trigger_sound = ctx.video_processor.trigger_sound
            
            # 2. UI ì—…ë°ì´íŠ¸
            if pred == 'good':
                status_ph.markdown(f"<div class='good-text'>ìƒíƒœ: ì¢‹ìŒ ğŸ˜Š</div>", unsafe_allow_html=True)
            elif pred == 'mild':
                status_ph.markdown(f"<div class='mild-text'>ìƒíƒœ: ì£¼ì˜ ğŸ˜</div>", unsafe_allow_html=True)
            else:
                status_ph.markdown(f"<div class='severe-text'>ìƒíƒœ: ì‹¬ê° ğŸ¢</div>", unsafe_allow_html=True)
            
            # í™•ë¥  ë°”
            st.sidebar.markdown("### ì„¸ë¶€ í™•ë¥ ")
            st.sidebar.progress(probs.get('good', 0), text=f"Good: {int(probs.get('good', 0)*100)}%")
            st.sidebar.progress(probs.get('mild', 0), text=f"Mild: {int(probs.get('mild', 0)*100)}%")
            st.sidebar.progress(probs.get('severe', 0), text=f"Severe: {int(probs.get('severe', 0)*100)}%")

            # 3. ì†Œë¦¬ ì¬ìƒ (JavaScript íŠ¸ë¦¬ê±°)
            if trigger_sound:
                # ìë°”ìŠ¤í¬ë¦½íŠ¸ í•¨ìˆ˜ í˜¸ì¶œ (playAlert)
                sound_ph.markdown("""
                    <script>
                    playAlert();
                    </script>
                    """, unsafe_allow_html=True)
                # í†µê³„ ëˆ„ì 
                st.session_state.total_severe_time += 0.1 # ëŒ€ëµì ì¸ ë£¨í”„ ì‹œê°„ ë”í•˜ê¸°
            else:
                sound_ph.empty()

            # 4. ì‹œê°„ í†µê³„ ì—…ë°ì´íŠ¸
            elapsed_time = int(time.time() - st.session_state.start_time)
            mins, secs = divmod(elapsed_time, 60)
            
            severe_mins, severe_secs = divmod(int(st.session_state.total_severe_time), 60)
            
            timer_ph.markdown(f"""
                <div class='stat-box'>
                    <div class='stat-title'>ì´ ì‚¬ìš© ì‹œê°„</div>
                    <div class='stat-value'>{mins}ë¶„ {secs}ì´ˆ</div>
                </div>
            """, unsafe_allow_html=True)
            
            severe_timer_ph.markdown(f"""
                <div class='stat-box'>
                    <div class='stat-title' style='color: #e74c3c;'>ë‚˜ìœ ìì„¸ ëˆ„ì </div>
                    <div class='stat-value' style='color: #e74c3c;'>{severe_mins}ë¶„ {severe_secs}ì´ˆ</div>
                </div>
            """, unsafe_allow_html=True)

        time.sleep(0.1)
